---
layout: post
title: 闢謠地獄累死小編 杜奕瑾：用AI助民眾了解認知操作
date: 2023-10-17 04:07:00.000000000 +00:00
link: https://www.cna.com.tw/news/ait/202310170092.aspx
categories: cna
---

<p>（中央社記者吳家豪台北17日電）台灣人工智慧實驗室創辦人杜奕瑾指出，訊息操作自2018年開始日益增多。以雞蛋事件為例，如果仍只是讓政府部門對進口雞蛋爭議篇篇立即回應，只會累死小編。</p><p>他認為，建立數位素養、讓民眾知道認知操作的狀況，並使用可信任的資訊平台來獲取訊息，可以讓民眾更有效率接收到完整訊息，避免被片面資訊誤導。</p><p>面對台灣各社群平台充斥許多訊息操作，杜奕瑾接受中央社記者視訊採訪時表示，台灣人工智慧實驗室（Taiwan AI Labs）發現，從2018年關西機場事件開始，面對假消息都以事實查核作為解方。但之後越來越多訊息操作的戰場往往不在於消息真假，而是刻意運用大量的誤導讓大眾對目標對象產生喜惡，原因是生成式科技以及大型語言模型（LLM）已被用來生成假帳號和散播操作消息。</p><p>杜奕瑾指出，事實查核必須經過嚴謹冗長的查證過程，而就算查完一個消息，又會有10個以上的假訊息冒出來，「基本上是查不完的」。另一方面自從全球建立事實查核組織的網絡後，訊息操作的走向常常不是造假而是誤導。如今生成式AI已經被拿來生成頭像和言論、幾可亂真。認知攻擊已經使用人工智慧（AI），防守網路攻擊也必須透過人工智慧去瞭解訊息的操作。</p><p>●認知操作見縫插針 放大社會不信任感</p><p>面對認知戰步步進逼，杜奕瑾直言，現階段作法應是揭露認知操作策略，讓民眾知道哪些議題被不正常的放大；認知操作就像「見縫插針」，快速把社會變得互相不信任，難以得出一致性結論，也更難弭平彼此之間的不同意見。</p><p>杜奕瑾表示，認知操作在台灣，常見同時鎖定2個不同陣營同時激化攻擊，並非為了要攻擊或吹捧特定對象，而是嘗試摧毀台灣過去的社會氛圍，進一步放大仇恨和偏見，變成彼此不信任的環境，以便後續有更好操作空間。</p><p>杜奕瑾強調，在生成式AI當道的時代，民主國家社交媒體都湧入大量的操作帳號，流入的訊息對一般人已經多到看不完、聽不完。有別於傳統要求平台做事實查核的方法，Taiwan AI Labs透過大型語言模型，讓所有網路意見都可以被AI理解與分析，同時教導民眾如何藉由使用新生成式科技，了解如何產生幾可亂真的內容，並辨識協同訊號，透過人工智慧再加上大型語言模型去分析和透視操作手法。</p><p>杜奕瑾指出，透過AI工具和演算法，可以協助民眾更有效率看完、讀完所有訊息，有助於突破同溫層；且倘若有訊息被操作，Taiwan AI Labs所打造的非營利訊息平台，也會揭露、標示，提醒使用者可以主動觀察某些仇恨言論是否被放大、使用者是否獲得全面性的消息，以免被片面資訊或不對稱翻譯所誤導。</p><p>●聲量大於真相年代  運用AI突破同溫層辨別認知操作</p><p>杜奕瑾說，以往政府部門面對假消息，原則是趕快回應或澄清，但從這次農業部進口雞蛋事件的相關訊息操作來看，政府部門馬上回應效果有限，只會累死小編，而民眾不見得買單。</p><p>他分析，當政府部門回應假消息後，協同帳號群會根據回應內容、生出更多的攻擊。聲量大於真相的年代，民眾的觀感更容易透過訊息操作被塑造，進而不信任官方的回應；政府部門在闢謠時，不能只針對事件本身，還要進一步揭露訊息操作手法，從行為模式分析操作策略教導大家辨識哪些誤導正在發生。</p><p>杜奕瑾說，協同帳號可分：造謠者、假中立者、傳播者與粉絲經營者等4種類型，分工細密，且在網路上「共進共出」、互相呼應的協同團體，會因社群平台特性而有不同的操作手法。</p><p>他指出，在臉書的假消息傳播者，現在會把假中立者的內容，散播到原本就有很大聲量的社群，例如運動社群或知名品牌粉絲頁；在PTT的操作手法，則是先貼出新聞，同時藉由不斷回文或針對新聞底下前幾則推文帶風向。</p><p>YouTube的協同操作行為又不太一樣。杜奕瑾說，Taiwan AI Labs發現最近有很多中國網紅在YouTube成立頻道，屬於訊息傳播者，協同帳號群會設法捧紅這些YouTuber，並在影片內容置入特定觀念，例如中國可以解決相關問題、生產雞蛋的方法很先進，進而偽裝成客觀或中立的意見進行論述。（編輯：潘羿菁、楊蘭軒）1121017</p>
